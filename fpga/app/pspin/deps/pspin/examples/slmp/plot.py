#!/usr/bin/env python3

import csv
import argparse
import re
import numpy as np
import scipy.stats as st
import pandas as pd
import sys
from math import ceil
from itertools import chain, product
from os import listdir
from os.path import isfile, join

import matplotlib.pyplot as plt
import matplotlib.text as mtext
import matplotlib.transforms as transforms
from matplotlib.colors import LogNorm, BoundaryNorm
from matplotlib.ticker import FuncFormatter, ScalarFormatter
from matplotlib.cm import ScalarMappable
from matplotlib import container, colormaps
import seaborn as sns

from si_prefix import si_format

from plot_lib import *

parser = argparse.ArgumentParser(
    prog='plot.py',
    description='Plot data generated by the slmp file transfer benchmark',
    epilog='Report bugs to Pengcheng Xu <pengxu@ethz.ch>.'
)

parser.add_argument('--data_root', help='root of the CSV files from the SLMP benchmark', default=None)
parser.add_argument('--query', action='store_true', help='query data interactively')

args = parser.parse_args()

dat_pkl = 'data.pkl'
loss_pkl = 'loss.pkl'

slmp_payload_size = 1462 // 4 * 4

def consume_trials(wnd_sz, tc, trials):
    # we do not distinguish host dma and notification here
    # since host is notified on every write
    def append_row(len, wnd_sz, tc, elapsed, lost, cycles, msg, pkt, dma, notification, headtail):
        global data
        num_pkts = ceil(len / slmp_payload_size)
        tputs = [len * 8 / 1e6 / e for e in elapsed]

        # 95% confidence interval
        if not tputs:
            tputs = [0]

        tput_median = np.median(tputs)
        if tputs.__len__() > 1:
            bootstrap_ci = st.bootstrap((tputs,), np.median, confidence_level=0.95, method='percentile')
            ci_lo, ci_hi = bootstrap_ci.confidence_interval
        else:
            ci_lo, ci_hi = tput_median, tput_median

        real_pkt = cycles_to_us((pkt - dma) * num_pkts / num_hpus)
        real_dma = cycles_to_us((dma - cycles) * num_pkts / num_hpus)

        real_headtail = cycles_to_us(headtail - cycles * 2.5)
        real_notification = cycles_to_us(notification - cycles)
        all_cycles = cycles_to_us(cycles * (4 * num_pkts / num_hpus + 7))

        el_mean = np.mean(elapsed)

        sender = el_mean - real_headtail - real_pkt - real_dma - real_notification - all_cycles

        tlen = tputs.__len__()
        if lost == 1:
            # suppress system instability
            loss_ratio = 0
        else:
            loss_ratio = lost / (tlen + lost)

        entry = pd.DataFrame.from_dict({
            'len': [len],
            'wnd_sz': [wnd_sz],
            'tc': [tc],
            'elapsed': [elapsed],
            'pkt': [pkt],
            'headtail': [headtail],
            'msg': [msg],
            'dma': [dma],
            'notification': [notification],
            'cycles': [cycles],
            # derived
            'tput': [tput_median],
            'tput_lo': [tput_median - ci_lo],
            'tput_hi': [ci_hi - tput_median],
            'loss_ratio': [loss_ratio],
            # normalised components
            'pkt_r': [real_pkt / el_mean],
            'headtail_r': [real_headtail / el_mean],
            'dma_r': [real_dma / el_mean],
            'notification_r': [real_notification / el_mean],
            'cycles_r': [all_cycles / el_mean],
            'sender_r': [sender / el_mean],
        })
        data = pd.concat([data, entry], ignore_index=True)

    for l in trials:
        prefix = f'{l}-{wnd_sz}-{tc}'

        try:
            with open(join(args.data_root, f'{prefix}.csv'), 'r') as f:
                reader = csv.reader(f)
                assert next(reader) == ['cycles', 'msg', 'pkt', 'dma', 'notification', 'headtail']
                handler_values = [cycles_to_us(float(x)) for x in next(reader)]
        except (FileNotFoundError, StopIteration) as e:
            print(f'Warning: exception consuming {prefix}: {e}')
            continue

        with open(join(args.data_root, f'{prefix}-sender.txt'), 'r') as f:
            lines = f.readlines()

        el = list(filter(lambda l: l.find('Elapsed: ') != -1, lines))
        timeouts = list(filter(lambda l: l.find('Timed out!') != -1, lines))

        elapsed_values = [float(l.split(' ')[1]) for l in el]

        append_row(int(l), wnd_sz, tc, elapsed_values, len(timeouts), *handler_values)

def gen_trial(start, end, step=2):
    x = start
    while x <= end:
        yield x
        x *= step

window_sizes = [*gen_trial(1, 1024, step=4), 512]
num_threads = [*gen_trial(1, 64)]

plot_curated = False
window_sizes_curated = [1, 4, 16, 250]
num_threads_curated = [1, 4, 16, 250]

if plot_curated:
    window_sizes_sel = window_sizes_curated
    num_threads_sel = num_threads_curated
else:
    window_sizes_sel = window_sizes
    num_threads_sel = num_threads

indices = [
        'cycles_r',
        'pkt_r',
        'headtail_r',
        'dma_r',
        'notification_r',
        'sender_r',
]

if args.data_root:
    trials = list(gen_trial(100, 128 * 1024 * 1024))

    data = pd.DataFrame(columns=[
        'len',
        'wnd_sz',
        'tc',
        'elapsed',
        'pkt',
        'headtail',
        'msg',
        'dma',
        'notification',
        'cycles',
        # derived
        'tput',
        'tput_lo',
        'tput_hi',
    ] + indices)

    for wnd_sz, tc in product(window_sizes, num_threads):
        consume_trials(wnd_sz, tc, trials)

    data['len'] = data['len'].astype(float)
    data.to_pickle(dat_pkl)

dp: pd.DataFrame = pd.read_pickle(dat_pkl)
if args.query:
    import code
    code.InteractiveConsole(locals=globals()).interact()
    sys.exit(0)

set_style()

# Throughput
fig, axes = plt.subplots(2, 2, figsize=figsize(1.05))

# lb = TitledLegendBuilder()
for i in range(2):
    for j in range(2):
        ax = axes[j][i]

        ax.sharey(axes[0][0])

        ylabel = 'Throughput (Mbps)'

        if j == 0:
            ax.set_xscale('log')
            x = 'len'
            xlabel = 'File Length (B)'
            x_formatter = FuncFormatter(lambda x, pos: si_format(x, precision=0))
            ax.xaxis.set_major_formatter(x_formatter)
        else:
            ax.set_xscale('log', base=2)
            ax.xaxis.set_major_formatter(ScalarFormatter())
            ax.xaxis.set_minor_formatter(ScalarFormatter())
            if i == 0:
                x = 'wnd_sz'
                xlabel = 'Window Size (#Packets)'
                ax.set_xbound(1, 1024)
            else:
                x = 'tc'
                xlabel = 'Thread Count'
                ax.set_xbound(1, 64)

        if i == 0:
            legends = num_threads_sel
            legend_cid = 'tc'
            legend_name = '#Threads'
            # cm_name = 'winter_r'
            cm_name = 'brg'
        else:
            legends = window_sizes_sel
            legend_cid = 'wnd_sz'
            legend_name = 'Window'
            # cm_name = 'autumn_r'
            cm_name = 'jet'

        norm = LogNorm(min(legends), max(legends))
        cmap = colormaps.get_cmap(cm_name)
        boundaries = [0]

        for lbl in legends:
            color = cmap(norm(lbl))
            boundaries.append(lbl)

            trial = dp[(dp[legend_cid] == lbl)]
            idx = trial.groupby(x)['tput'].transform(max) == trial['tput']
            trial = trial[idx].sort_values(x)

            trial = trial[trial['loss_ratio'] <= .9] # 20 out of 200
            ax.errorbar(x, 'tput', yerr=(trial['tput_lo'], trial['tput_hi']), data=trial, color=color, ecolor='black')

            # lb.push(legend_name, lbl, ax.lines[-1])

        ax.set_xlabel(xlabel)
        ax.set_ylabel(ylabel)

        # create discrete colorbar
        boundaries.sort()
        norm = BoundaryNorm(boundaries, cmap.N)
        cbar = fig.colorbar(ScalarMappable(norm=norm, cmap=cmap), ax=ax)
        cbar.ax.set_title(legend_name)
        # cbar.ax.set_yscale('log', base=2) # ONLY needed for LogNorm
        y_formatter = FuncFormatter(lambda y, pos: str(int(y)))
        cbar.ax.yaxis.set_major_formatter(y_formatter)

        # move the label to center
        # https://stackoverflow.com/a/49449590/5520728
        dx = 0/72.; dy = -8/72. 
        offset = transforms.ScaledTranslation(dx, dy, fig.dpi_scale_trans)
        for idx, label in enumerate(cbar.ax.yaxis.get_ticklabels()):
            label.set_transform(label.get_transform() + offset)
            if not idx:
                # hide 0
                label.set_visible(False)
    
        # adapted from ax.label_outer()
        if i == 1:
            for label in ax.get_yticklabels(which="both"):
                label.set_visible(False)
            ax.get_yaxis().get_offset_text().set_visible(False)
            ax.set_ylabel("")

        # iperf theoretical data
        iperf_dat = [9.22, 10.4, 8.36, 8.49, 7.75, 8.66, 7.63, 8.60, 8.70, 7.89, 7.92, 8.16, 7.87, 8.07, 7.31, 7.78]
        iperf_dat = [x * 1000 for x in iperf_dat] # convert to Mbps

        dt_max = 623

        iperf_line = ax.axhline(y=np.mean(iperf_dat), linestyle='--', color='purple')
        dt_line = ax.axhline(dt_max, linestyle='-.', color='orchid')

        ax.grid(which='minor', alpha=0.2)
        ax.grid(which='major', alpha=0.5)

'''
lb.push('', 'IPerf3', iperf_line)
lb.draw(fig)
'''
fig.legend([iperf_line, dt_line], ['IPerf3', 'Datatypes'], bbox_to_anchor=[.5, 1], loc='upper center', ncol=2)
fig.tight_layout(rect=[0, 0, 1, .95])
fig.savefig('slmp-tput.pdf')

fig, axes = plt.subplots(1, 2, sharey=True, figsize=figsize(2.1))

for i in range(2):
    ax = axes[i]

    ax.set_ylabel('Failure Rate')
    y_formatter = FuncFormatter(lambda y, pos: f'{int(y * 100)}%')
    ax.yaxis.set_major_formatter(y_formatter)
    ax.set_ybound(0, 1)

    ax.set_xscale('log', base=2)
    ax.xaxis.set_major_formatter(ScalarFormatter())
    ax.xaxis.set_minor_formatter(ScalarFormatter())
    if i == 0:
        x = 'wnd_sz'
        xlabel = 'Window Size (#Packets)'
        ax.set_xbound(1, 1024)
    else:
        x = 'tc'
        xlabel = 'Thread Count'
        ax.set_xbound(1, 64)

    if i == 0:
        legends = num_threads_sel
        legend_cid = 'tc'
        legend_name = '#Threads'
        # cm_name = 'winter_r'
        cm_name = 'brg'
    else:
        legends = window_sizes_sel
        legend_cid = 'wnd_sz'
        legend_name = 'Window'
        # cm_name = 'autumn_r'
        cm_name = 'jet'

    ax.set_xlabel(xlabel)
    ax.label_outer()

    norm = LogNorm(min(legends), max(legends))
    cmap = colormaps.get_cmap(cm_name)

    boundaries = [0]

    for lbl in legends:
        color = cmap(norm(lbl))
        boundaries.append(lbl)

        trial = dp[(dp[legend_cid] == lbl)]
        idx = trial.groupby(x)['loss_ratio'].transform(max) == trial['loss_ratio']
        trial = trial[idx].sort_values(x)

        ax.plot(x, 'loss_ratio', data=trial, color=color)

    # create discrete colorbar
    boundaries.sort()
    norm = BoundaryNorm(boundaries, cmap.N)
    cbar = fig.colorbar(ScalarMappable(norm=norm, cmap=cmap), ax=ax)
    cbar.ax.set_title(legend_name)
    # cbar.ax.set_yscale('log', base=2) # ONLY needed for LogNorm
    y_formatter = FuncFormatter(lambda y, pos: str(int(y)))
    cbar.ax.yaxis.set_major_formatter(y_formatter)

    # move the label to center
    # https://stackoverflow.com/a/49449590/5520728
    dx = 0/72.; dy = -8/72. 
    offset = transforms.ScaledTranslation(dx, dy, fig.dpi_scale_trans)
    for idx, label in enumerate(cbar.ax.yaxis.get_ticklabels()):
        label.set_transform(label.get_transform() + offset)
        if not idx:
            # hide 0
            label.set_visible(False)
    
    ax.grid(which='minor', alpha=0.2)
    ax.grid(which='major', alpha=0.5)

fig.savefig('slmp-loss.pdf')